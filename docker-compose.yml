# Development Docker Compose
# For production deployment with pre-built images, use: docker-compose.prod.yml
# See README.md for deployment options

services:
  # ollama:
  #   image: ollama/ollama:latest
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ./ollama_data:/root/.ollama
  #   command: serve

  weaviate:
    profiles: ["local-weaviate"]
    image: cr.weaviate.io/semitechnologies/weaviate:1.33.0
    ports:
      - "8080:8080"
      - "50051:50051"
    volumes:
      - ./weaviate_data:/var/lib/weaviate
    restart: on-failure:0
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      PERSISTENCE_DATA_PATH: "/var/lib/weaviate"
      DEFAULT_VECTORIZER_MODULE: "none"
      ENABLE_MODULES: "reranker-voyageai"
      CLUSTER_HOSTNAME: "node1"

  # backend:
  #   build:
  #     context: ./backend
  #   ports:
  #     - "8001:8001"
  #   volumes:
  #     - ./backend:/app
  #   depends_on:
  #     - weaviate
  #     - ollama
  #   environment:
  #     - PYTHONUNBUFFERED=1
  #   env_file:
  #     - .env
  #   command: uvicorn main:app --host 0.0.0.0 --port 8001 --reload

  backend-semantic:
    # Development: Build from source with hot-reload
    build:
      context: ./backend
      dockerfile: Dockerfile.backend
    # Production alternative: Use pre-built image from GHCR
    # Uncomment the line below and comment out 'build' section:
    # image: ghcr.io/maskys/roam-embeddings-search-backend:latest
    ports:
      - "8002:8000"
    volumes:
      - ./backend:/app
    depends_on:
      # Weaviate is optional when using Weaviate Cloud or remote instances.
      # Start it via profile 'local-weaviate' and include 'weaviate' in the up command when needed.
      # - weaviate
      # - ollama
      - chunker
    environment:
      - PYTHONUNBUFFERED=1
    env_file:
      - .env
    command: uvicorn services.search_service:app --host 0.0.0.0 --port 8000 --reload

  chunker:
    # Development: Build from source
    build:
      context: ./backend
      dockerfile: Dockerfile.chunker
    # Production alternative: Use pre-built image from GHCR
    # Uncomment the line below and comment out 'build' section:
    # image: ghcr.io/maskys/roam-embeddings-search-chunker:latest
    container_name: roam-test-chunker
    ports:
      - "8003:8003"
    volumes:
      - ./backend:/app
    environment:
      - PYTHONUNBUFFERED=1
      # Use voyageai (default) for low memory footprint, or granite for local model
      - CHUNKER_EMBEDDING_PROVIDER=${CHUNKER_EMBEDDING_PROVIDER:-voyageai}
      - CHUNKER_VOYAGE_MODEL=${CHUNKER_VOYAGE_MODEL:-voyage-3-lite}
      - VOYAGE_API_KEY=${VOYAGEAI_API_KEY}
      - CHUNKER_MODEL=${CHUNKER_MODEL:-ibm-granite/granite-embedding-small-english-r2}
      - CHUNKER_THRESHOLD=${CHUNKER_THRESHOLD:-0.6}
      - CHUNKER_CHUNK_SIZE=${CHUNKER_CHUNK_SIZE:-800}
      - CHUNKER_SKIP_WINDOW=${CHUNKER_SKIP_WINDOW:-1}
      - CHUNKER_MIN_CHUNK_SIZE=${CHUNKER_MIN_CHUNK_SIZE:-50}
    command: uvicorn services.chunker_service:app --host 0.0.0.0 --port 8003
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
